{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDA3020 Homework 2\n",
    "### Instructions:\n",
    "- Follow the notebook and complete the code cells marked as TODO\n",
    "- Ensure your code runs successfully until the end of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========== data info ============ #\n",
      "train validation data: (1000, 100)\n",
      "train validation label: (1000,)\n",
      "test data: (400, 100)\n",
      "test label: (400,)\n",
      "# ================================= #\n"
     ]
    }
   ],
   "source": [
    "from os import path as osp\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "def load_data():\n",
    "\n",
    "    data_dir = './data'\n",
    "    train_val_data_path = osp.join(data_dir, 'train_validation_data.npy')\n",
    "    train_val_label_path = osp.join(data_dir, 'train_validation_label.npy')\n",
    "    test_data_path = osp.join(data_dir, 'test_data.npy')\n",
    "    test_label_path = osp.join(data_dir, 'test_label.npy')\n",
    "\n",
    "    train_val_data = np.load(train_val_data_path)\n",
    "    train_val_label = np.load(train_val_label_path)\n",
    "    test_data = np.load(test_data_path)\n",
    "    test_label = np.load(test_label_path)\n",
    "    return train_val_data, train_val_label, test_data, test_label\n",
    "\n",
    "\n",
    "train_validation_data, train_validation_label, test_data, test_label = load_data()\n",
    "\n",
    "print(f'# ========== data info ============ #')\n",
    "print(f'train validation data: {train_validation_data.shape}')\n",
    "print(f'train validation label: {train_validation_label.shape}')\n",
    "print(f'test data: {test_data.shape}')\n",
    "print(f'test label: {test_label.shape}')\n",
    "print(f'# ================================= #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split for K-fold Cross-validation\n",
    "\n",
    "def train_validation_split(K, train_val_data, train_val_label):\n",
    "\n",
    "    class0_ind = np.where(train_val_label == 0)[0]\n",
    "    class1_ind = np.where(train_val_label == 1)[0]\n",
    "\n",
    "    # Get data of different classes\n",
    "    data_0 = train_val_data[class0_ind]\n",
    "    label_0 = train_val_label[class0_ind]\n",
    "    data_1 = train_val_data[class1_ind]\n",
    "    label_1 = train_val_label[class1_ind]\n",
    "\n",
    "    # Random shuffle of current data: gurantee unbias folds  \n",
    "    p0 = np.random.permutation(len(data_0))\n",
    "    data_0 = data_0[p0]\n",
    "    label_0 = label_0[p0]\n",
    "\n",
    "    p1 = np.random.permutation(len(data_1))\n",
    "    data_1 = data_1[p1]\n",
    "    label_1 = label_1[p1]  \n",
    "\n",
    "    # Split each class into K folds  \n",
    "    n_0 = len(data_0)\n",
    "    n_1 = len(data_1) \n",
    "\n",
    "    # Get fold size for both class 0 & 1 to gurantee consistency\n",
    "    fold_0_size = (n_0//K) * np.ones(K,dtype = int)\n",
    "    fold_1_size = (n_1//K) * np.ones(K,dtype = int)  \n",
    "        ## Remainder addition, if exists \n",
    "    \n",
    "    # Create folds for 0 & 1, at last combine together\n",
    "    data_fold_0 = []\n",
    "    data_fold_1 = []\n",
    "    label_fold_0 = []\n",
    "    label_fold_1 = []\n",
    "\n",
    "    current_ind = 0\n",
    "    \n",
    "    for i in fold_0_size:\n",
    "        data_fold_0.append(data_0[current_ind: (current_ind + i)])  # i-th fold inclusion as sublist \n",
    "        label_fold_0.append(label_0[current_ind:(current_ind+i)])\n",
    "        current_ind = current_ind + i\n",
    "\n",
    "    current_ind = 0\n",
    "\n",
    "    for i in fold_1_size:\n",
    "        data_fold_1.append(data_1[current_ind: (current_ind + i)]) \n",
    "        label_fold_1.append(label_1[current_ind:(current_ind+i)])\n",
    "        current_ind = current_ind + i\n",
    "\n",
    "    # Final combination into training & validation sets\n",
    "\n",
    "    train_datas = []\n",
    "    train_labels = []\n",
    "    val_datas = []\n",
    "    val_labels = []\n",
    "    \n",
    "    # combine of feature & label\n",
    "    for i in range(K):\n",
    "        # i-th Validation set\n",
    "        val_data = np.vstack((data_fold_0[i], data_fold_1[i]))\n",
    "        val_label = np.hstack((label_fold_0[i], label_fold_1[i]))\n",
    "\n",
    "        # except i-th as training set\n",
    "        train_data_0 = np.vstack([data_fold_0[j] for j in range(K) if j != i])\n",
    "        train_label_0 = np.hstack([label_fold_0[j] for j in range(K) if j != i])\n",
    "        \n",
    "        train_data_1 = np.vstack([data_fold_1[j] for j in range(K) if j != i])\n",
    "        train_label_1 = np.hstack([label_fold_1[j] for j in range(K) if j != i])       \n",
    "           \n",
    "        # Combine class 0 & class 1ï¼Œ feature & label separately\n",
    "        train_data = np.vstack((train_data_0, train_data_1))\n",
    "        train_label = np.hstack((train_label_0, train_label_1))\n",
    "        \n",
    "        # Shuffle for final output\n",
    "        p_train = np.random.permutation(len(train_data))\n",
    "        train_data = train_data[p_train]\n",
    "        train_label = train_label[p_train]\n",
    "        \n",
    "        p_val = np.random.permutation(len(val_data))\n",
    "        val_data = val_data[p_val]\n",
    "        val_label = val_label[p_val]\n",
    "        \n",
    "        # Append to lists\n",
    "        train_datas.append(train_data)\n",
    "        train_labels.append(train_label)\n",
    "        val_datas.append(val_data)\n",
    "        val_labels.append(val_label)\n",
    "    \n",
    "    return train_datas, train_labels, val_datas, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "  Training Data Shape: (800, 100)\n",
      "  Training Labels Shape: (800,)\n",
      "  Validation Data Shape: (200, 100)\n",
      "  Validation Labels Shape: (200,)\n",
      "Fold 2:\n",
      "  Training Data Shape: (800, 100)\n",
      "  Training Labels Shape: (800,)\n",
      "  Validation Data Shape: (200, 100)\n",
      "  Validation Labels Shape: (200,)\n",
      "Fold 3:\n",
      "  Training Data Shape: (800, 100)\n",
      "  Training Labels Shape: (800,)\n",
      "  Validation Data Shape: (200, 100)\n",
      "  Validation Labels Shape: (200,)\n",
      "Fold 4:\n",
      "  Training Data Shape: (800, 100)\n",
      "  Training Labels Shape: (800,)\n",
      "  Validation Data Shape: (200, 100)\n",
      "  Validation Labels Shape: (200,)\n",
      "Fold 5:\n",
      "  Training Data Shape: (800, 100)\n",
      "  Training Labels Shape: (800,)\n",
      "  Validation Data Shape: (200, 100)\n",
      "  Validation Labels Shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "K = 5\n",
    "\n",
    "train_datas, train_labels, val_datas, val_labels = train_validation_split(K, train_validation_data, train_validation_label)\n",
    "\n",
    "for i in range(K):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"  Training Data Shape: {train_datas[i].shape}\")   \n",
    "    print(f\"  Training Labels Shape: {train_labels[i].shape}\") \n",
    "    print(f\"  Validation Data Shape: {val_datas[i].shape}\")   \n",
    "    print(f\"  Validation Labels Shape: {val_labels[i].shape}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the training and validation set for the two classes are splitted evenly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics\n",
    "\n",
    "def eva_precision(true_label, pred_label, _class):\n",
    "    \n",
    "    TP = np.sum((pred_label == _class) & (true_label == _class))\n",
    "    FP = np.sum((pred_label == _class) & (true_label != _class))\n",
    "    if TP + FP == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = TP / (TP + FP)\n",
    "    return precision\n",
    "\n",
    "def eva_recall(true_label, pred_label, _class):\n",
    "\n",
    "    TP = np.sum((pred_label == _class) & (true_label == _class))\n",
    "    FN = np.sum((pred_label != _class) & (true_label == _class))\n",
    "    if TP + FN == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = TP / (TP + FN)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def eva_f1(true_label, pred_label, _class):\n",
    "    precision = eva_precision(true_label, pred_label, _class)\n",
    "    recall = eva_recall(true_label, pred_label, _class)\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def eva_accuracy(true_label, pred_label):\n",
    "    correct_predictions = np.sum(pred_label == true_label)\n",
    "    total_predictions = len(true_label)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "def eva_auroc(true_label, pred_label):\n",
    "    desc_pred_indices = np.argsort(-pred_label)\n",
    "    sorted_true_labels = true_label[desc_pred_indices] \n",
    "\n",
    "    P = np.sum(true_label == 1)\n",
    "    N = np.sum(true_label == 0)\n",
    "    TPR = np.cumsum(sorted_true_labels == 1) / P  # True Positive Rate\n",
    "    FPR = np.cumsum(sorted_true_labels == 0) / N  # False Positive Rate   \n",
    "    \n",
    "    # Add (0,0) as the beginning of roc curve\n",
    "    TPR = np.insert(TPR, 0, 0)\n",
    "    FPR = np.insert(FPR, 0, 0)\n",
    "\n",
    "    # Calculate AUROC using the trapezoidal rule\n",
    "    auroc = np.trapz(TPR, FPR)\n",
    "    return auroc\n",
    "\n",
    "def evaluation(true_label, pred_label, _class):\n",
    "\n",
    "    precision = eva_precision(true_label, pred_label, _class)\n",
    "    recall = eva_recall(true_label, pred_label, _class)\n",
    "    f1 = eva_f1(true_label, pred_label, _class)\n",
    "    accuracy = eva_accuracy(true_label, pred_label)\n",
    "    auroc = eva_auroc(true_label, pred_label)\n",
    "\n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1, 'accuracy': accuracy, 'auroc': auroc}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 Fine Tuning Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_output = [[] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ======================= 1-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: {'penalty': 'l1'}\n",
      "F1 (Val set of Class-0): 0.9557\n",
      "F1 (Val set of Class-1): 0.9543\n",
      "# ======================= 2-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: {'penalty': 'l1'}\n",
      "F1 (Val set of Class-0): 0.9333\n",
      "F1 (Val set of Class-1): 0.9263\n",
      "# ======================= 3-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: {'penalty': 'l1'}\n",
      "F1 (Val set of Class-0): 0.9347\n",
      "F1 (Val set of Class-1): 0.9353\n",
      "# ======================= 4-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: {'penalty': 'l1'}\n",
      "F1 (Val set of Class-0): 0.9652\n",
      "F1 (Val set of Class-1): 0.9648\n",
      "# ======================= 5-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: {'penalty': 'l1'}\n",
      "F1 (Val set of Class-0): 0.9293\n",
      "F1 (Val set of Class-1): 0.9307\n"
     ]
    }
   ],
   "source": [
    "# model training and hyper-parameters fine-tuning\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "K = 5\n",
    "\n",
    "# hyper-parameter for logistic regression\n",
    "hyper_parameters_logistic_regression = {\n",
    "\n",
    "    # TODO: please choose different values to tune the model\n",
    "    'penalty': 'l1' # ['l1', 'l2']\n",
    "}\n",
    "\n",
    "\n",
    "# obtain cross-validation set\n",
    "train_datas, train_labels, validation_datas, validation_labels = train_validation_split(K, train_validation_data, train_validation_label)\n",
    "\n",
    "\n",
    "for i, (train_data, train_label, validation_data, validation_label) in enumerate(zip(train_datas, train_labels, validation_datas, validation_labels)):\n",
    "\n",
    "    print(f'# ======================= {i + 1}-th time validation ======================= #')\n",
    "\n",
    "    # logistic regression\n",
    "\n",
    "    print(f'Algorithm: [logistic regression] =========================')\n",
    "    print(f'hyper-parameter: {hyper_parameters_logistic_regression}')\n",
    "    lr_model = LogisticRegression(solver='liblinear', **hyper_parameters_logistic_regression).fit(train_data, train_label)\n",
    "\n",
    "    # performance evaluation on validation set for tuning hyper-parameters\n",
    "    pred_label = lr_model.predict(validation_data)\n",
    "    F1_0 = eva_f1(validation_label, pred_label, _class=0)\n",
    "    print(f'F1 (Val set of Class-0): {F1_0:.4f}')\n",
    "    F1_1 = eva_f1(validation_label, pred_label, _class=1)\n",
    "    print(f'F1 (Val set of Class-1): {F1_1:.4f}')\n",
    "\n",
    "    f1_lst = [F1_0,F1_1]\n",
    "    fold_output[i].append(f1_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ======================= 1-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: {'penalty': 'l2'}\n",
      "F1 (Val set of Class-0): 0.9463\n",
      "F1 (Val set of Class-1): 0.9436\n",
      "# ======================= 2-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: {'penalty': 'l2'}\n",
      "F1 (Val set of Class-0): 0.9159\n",
      "F1 (Val set of Class-1): 0.9032\n",
      "# ======================= 3-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: {'penalty': 'l2'}\n",
      "F1 (Val set of Class-0): 0.9406\n",
      "F1 (Val set of Class-1): 0.9394\n",
      "# ======================= 4-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: {'penalty': 'l2'}\n",
      "F1 (Val set of Class-0): 0.9697\n",
      "F1 (Val set of Class-1): 0.9703\n",
      "# ======================= 5-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: {'penalty': 'l2'}\n",
      "F1 (Val set of Class-0): 0.8889\n",
      "F1 (Val set of Class-1): 0.8911\n"
     ]
    }
   ],
   "source": [
    "# model training and hyper-parameters fine-tuning\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "K = 5\n",
    "\n",
    "# hyper-parameter for logistic regression\n",
    "hyper_parameters_logistic_regression = {\n",
    "\n",
    "    # TODO: please choose different values to tune the model\n",
    "    'penalty': 'l2' # ['l1', 'l2']\n",
    "}\n",
    "\n",
    "\n",
    "# obtain cross-validation set\n",
    "train_datas, train_labels, validation_datas, validation_labels = train_validation_split(K, train_validation_data, train_validation_label)\n",
    "\n",
    "\n",
    "for i, (train_data, train_label, validation_data, validation_label) in enumerate(zip(train_datas, train_labels, validation_datas, validation_labels)):\n",
    "\n",
    "    print(f'# ======================= {i + 1}-th time validation ======================= #')\n",
    "\n",
    "    # logistic regression\n",
    "\n",
    "    print(f'Algorithm: [logistic regression] =========================')\n",
    "    print(f'hyper-parameter: {hyper_parameters_logistic_regression}')\n",
    "    lr_model = LogisticRegression(solver='liblinear', **hyper_parameters_logistic_regression).fit(train_data, train_label)\n",
    "\n",
    "    # performance evaluation on validation set for tuning hyper-parameters\n",
    "    pred_label = lr_model.predict(validation_data)\n",
    "    F1_0 = eva_f1(validation_label, pred_label, _class=0)\n",
    "    print(f'F1 (Val set of Class-0): {F1_0:.4f}')\n",
    "    F1_1 = eva_f1(validation_label, pred_label, _class=1)\n",
    "    print(f'F1 (Val set of Class-1): {F1_1:.4f}')\n",
    "    f1_lst = [F1_0,F1_1]\n",
    "    fold_output[i].append(f1_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design a function to get the max indices for penalty value decision\n",
    "def find_max_avg_indices(lst):\n",
    "    max_indices = []\n",
    "    for sublist in lst:\n",
    "        max_index = 0\n",
    "        max_avg = float('-inf')\n",
    "        for index, subsublist in enumerate(sublist):\n",
    "            current_avg = sum(subsublist) / len(subsublist)\n",
    "            if current_avg > max_avg:\n",
    "                max_avg = current_avg\n",
    "                max_index = index\n",
    "        max_indices.append(max_index)\n",
    "    return max_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.9556650246305418, 0.9543147208121826],\n",
       "  [0.9463414634146342, 0.9435897435897437]],\n",
       " [[0.9333333333333333, 0.9263157894736842],\n",
       "  [0.9158878504672898, 0.9032258064516129]],\n",
       " [[0.9346733668341709, 0.9353233830845772],\n",
       "  [0.9405940594059405, 0.9393939393939393]],\n",
       " [[0.9651741293532338, 0.964824120603015],\n",
       "  [0.9696969696969697, 0.9702970297029702]],\n",
       " [[0.9292929292929293, 0.9306930693069307],\n",
       "  [0.888888888888889, 0.8910891089108911]]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 0]\n",
      "['l1', 'l1', 'l2', 'l2', 'l1']\n",
      "Observed from above results, we know the optimal hyper-paramters that have largest f1 score for both classes in each fold/split are: ['l1', 'l1', 'l2', 'l2', 'l1']\n"
     ]
    }
   ],
   "source": [
    "penalty_lst = ['l1','l2']\n",
    "print(find_max_avg_indices(fold_output))\n",
    "penalty_ind = find_max_avg_indices(fold_output)\n",
    "penalty = [penalty_lst[i] for i in penalty_ind]\n",
    "print(penalty)\n",
    "print(f'Observed from above results, we know the optimal hyper-paramters that have largest f1 score for both classes in each fold/split are: {penalty}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fold_output = [[] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ======================= 1-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 1e-05}\n",
      "F1 (Val set of Class-0): 0.9652\n",
      "F1 (Val set of Class-1): 0.9648\n",
      "# ======================= 2-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 1e-05}\n",
      "F1 (Val set of Class-0): 0.9154\n",
      "F1 (Val set of Class-1): 0.9146\n",
      "# ======================= 3-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 1e-05}\n",
      "F1 (Val set of Class-0): 0.9588\n",
      "F1 (Val set of Class-1): 0.9612\n",
      "# ======================= 4-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 1e-05}\n",
      "F1 (Val set of Class-0): 0.9703\n",
      "F1 (Val set of Class-1): 0.9697\n",
      "# ======================= 5-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 1e-05}\n",
      "F1 (Val set of Class-0): 0.9548\n",
      "F1 (Val set of Class-1): 0.9552\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter for SVM\n",
    "hyper_parameters_svm = {\n",
    "\n",
    "    # TODO: please choose different values to tune the model\n",
    "    'C': 1e-5# [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "\n",
    "}\n",
    "# obtain cross-validation set\n",
    "train_datas, train_labels, validation_datas, validation_labels = train_validation_split(K, train_validation_data, train_validation_label)\n",
    "\n",
    "\n",
    "for i, (train_data, train_label, validation_data, validation_label) in enumerate(zip(train_datas, train_labels, validation_datas, validation_labels)):\n",
    "\n",
    "    print(f'# ======================= {i + 1}-th time validation ======================= #')\n",
    "        # SVM\n",
    "\n",
    "    print(f'Algorithm: [SVM] =========================================')\n",
    "    print(f'hyper-parameter: {hyper_parameters_svm}')\n",
    "    svm_model = SVC(kernel='linear', **hyper_parameters_svm).fit(train_data, train_label)\n",
    "\n",
    "    # performance evaluation on validation set for tuning hyper-parameters\n",
    "    pred_label = svm_model.predict(validation_data)\n",
    "    F1_0 = eva_f1(validation_label, pred_label, _class=0)\n",
    "    \n",
    "    print(f'F1 (Val set of Class-0): {F1_0:.4f}')\n",
    "    F1_1 = eva_f1(validation_label, pred_label, _class=1)\n",
    "    \n",
    "    print(f'F1 (Val set of Class-1): {F1_1:.4f}')\n",
    "    f1_lst = [F1_0,F1_1]\n",
    "    fold_output[i].append(f1_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ======================= 1-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.0001}\n",
      "F1 (Val set of Class-0): 0.9453\n",
      "F1 (Val set of Class-1): 0.9447\n",
      "# ======================= 2-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.0001}\n",
      "F1 (Val set of Class-0): 0.9510\n",
      "F1 (Val set of Class-1): 0.9490\n",
      "# ======================= 3-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.0001}\n",
      "F1 (Val set of Class-0): 0.9548\n",
      "F1 (Val set of Class-1): 0.9552\n",
      "# ======================= 4-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.0001}\n",
      "F1 (Val set of Class-0): 0.9700\n",
      "F1 (Val set of Class-1): 0.9700\n",
      "# ======================= 5-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.0001}\n",
      "F1 (Val set of Class-0): 0.9600\n",
      "F1 (Val set of Class-1): 0.9600\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter for SVM\n",
    "hyper_parameters_svm = {\n",
    "\n",
    "    # TODO: please choose different values to tune the model\n",
    "    'C': 1e-4# [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "\n",
    "}\n",
    "# obtain cross-validation set\n",
    "train_datas, train_labels, validation_datas, validation_labels = train_validation_split(K, train_validation_data, train_validation_label)\n",
    "\n",
    "\n",
    "for i, (train_data, train_label, validation_data, validation_label) in enumerate(zip(train_datas, train_labels, validation_datas, validation_labels)):\n",
    "\n",
    "    print(f'# ======================= {i + 1}-th time validation ======================= #')\n",
    "        # SVM\n",
    "\n",
    "    print(f'Algorithm: [SVM] =========================================')\n",
    "    print(f'hyper-parameter: {hyper_parameters_svm}')\n",
    "    svm_model = SVC(kernel='linear', **hyper_parameters_svm).fit(train_data, train_label)\n",
    "\n",
    "    # performance evaluation on validation set for tuning hyper-parameters\n",
    "    pred_label = svm_model.predict(validation_data)\n",
    "    F1_0 = eva_f1(validation_label, pred_label, _class=0)\n",
    "    \n",
    "    print(f'F1 (Val set of Class-0): {F1_0:.4f}')\n",
    "    F1_1 = eva_f1(validation_label, pred_label, _class=1)\n",
    "    \n",
    "    print(f'F1 (Val set of Class-1): {F1_1:.4f}')\n",
    "    f1_lst = [F1_0,F1_1]\n",
    "    fold_output[i].append(f1_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ======================= 1-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 (Val set of Class-0): 0.9286\n",
      "F1 (Val set of Class-1): 0.9314\n",
      "# ======================= 2-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.001}\n",
      "F1 (Val set of Class-0): 0.9552\n",
      "F1 (Val set of Class-1): 0.9548\n",
      "# ======================= 3-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.001}\n",
      "F1 (Val set of Class-0): 0.9216\n",
      "F1 (Val set of Class-1): 0.9184\n",
      "# ======================= 4-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.001}\n",
      "F1 (Val set of Class-0): 0.9596\n",
      "F1 (Val set of Class-1): 0.9604\n",
      "# ======================= 5-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.001}\n",
      "F1 (Val set of Class-0): 0.9500\n",
      "F1 (Val set of Class-1): 0.9500\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter for SVM\n",
    "hyper_parameters_svm = {\n",
    "\n",
    "    # TODO: please choose different values to tune the model\n",
    "    'C': 1e-3# [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "\n",
    "}\n",
    "# obtain cross-validation set\n",
    "train_datas, train_labels, validation_datas, validation_labels = train_validation_split(K, train_validation_data, train_validation_label)\n",
    "\n",
    "\n",
    "for i, (train_data, train_label, validation_data, validation_label) in enumerate(zip(train_datas, train_labels, validation_datas, validation_labels)):\n",
    "\n",
    "    print(f'# ======================= {i + 1}-th time validation ======================= #')\n",
    "        # SVM\n",
    "\n",
    "    print(f'Algorithm: [SVM] =========================================')\n",
    "    print(f'hyper-parameter: {hyper_parameters_svm}')\n",
    "    svm_model = SVC(kernel='linear', **hyper_parameters_svm).fit(train_data, train_label)\n",
    "\n",
    "    # performance evaluation on validation set for tuning hyper-parameters\n",
    "    pred_label = svm_model.predict(validation_data)\n",
    "    F1_0 = eva_f1(validation_label, pred_label, _class=0)\n",
    "    \n",
    "    print(f'F1 (Val set of Class-0): {F1_0:.4f}')\n",
    "    F1_1 = eva_f1(validation_label, pred_label, _class=1)\n",
    "    \n",
    "    print(f'F1 (Val set of Class-1): {F1_1:.4f}')\n",
    "    f1_lst = [F1_0,F1_1]\n",
    "    fold_output[i].append(f1_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ======================= 1-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.01}\n",
      "F1 (Val set of Class-0): 0.9490\n",
      "F1 (Val set of Class-1): 0.9510\n",
      "# ======================= 2-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.01}\n",
      "F1 (Val set of Class-0): 0.9307\n",
      "F1 (Val set of Class-1): 0.9293\n",
      "# ======================= 3-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.01}\n",
      "F1 (Val set of Class-0): 0.9543\n",
      "F1 (Val set of Class-1): 0.9557\n",
      "# ======================= 4-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.01}\n",
      "F1 (Val set of Class-0): 0.9261\n",
      "F1 (Val set of Class-1): 0.9239\n",
      "# ======================= 5-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.01}\n",
      "F1 (Val set of Class-0): 0.9458\n",
      "F1 (Val set of Class-1): 0.9442\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter for SVM\n",
    "hyper_parameters_svm = {\n",
    "\n",
    "    # TODO: please choose different values to tune the model\n",
    "    'C': 1e-2# [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "\n",
    "}\n",
    "# obtain cross-validation set\n",
    "train_datas, train_labels, validation_datas, validation_labels = train_validation_split(K, train_validation_data, train_validation_label)\n",
    "\n",
    "\n",
    "for i, (train_data, train_label, validation_data, validation_label) in enumerate(zip(train_datas, train_labels, validation_datas, validation_labels)):\n",
    "\n",
    "    print(f'# ======================= {i + 1}-th time validation ======================= #')\n",
    "        # SVM\n",
    "\n",
    "    print(f'Algorithm: [SVM] =========================================')\n",
    "    print(f'hyper-parameter: {hyper_parameters_svm}')\n",
    "    svm_model = SVC(kernel='linear', **hyper_parameters_svm).fit(train_data, train_label)\n",
    "\n",
    "    # performance evaluation on validation set for tuning hyper-parameters\n",
    "    pred_label = svm_model.predict(validation_data)\n",
    "    F1_0 = eva_f1(validation_label, pred_label, _class=0)\n",
    "    \n",
    "    print(f'F1 (Val set of Class-0): {F1_0:.4f}')\n",
    "    F1_1 = eva_f1(validation_label, pred_label, _class=1)\n",
    "    \n",
    "    print(f'F1 (Val set of Class-1): {F1_1:.4f}')\n",
    "    f1_lst = [F1_0,F1_1]\n",
    "    fold_output[i].append(f1_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ======================= 1-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.1}\n",
      "F1 (Val set of Class-0): 0.9119\n",
      "F1 (Val set of Class-1): 0.9179\n",
      "# ======================= 2-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.1}\n",
      "F1 (Val set of Class-0): 0.9400\n",
      "F1 (Val set of Class-1): 0.9400\n",
      "# ======================= 3-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.1}\n",
      "F1 (Val set of Class-0): 0.9347\n",
      "F1 (Val set of Class-1): 0.9353\n",
      "# ======================= 4-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.1}\n",
      "F1 (Val set of Class-0): 0.9118\n",
      "F1 (Val set of Class-1): 0.9082\n",
      "# ======================= 5-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 0.1}\n",
      "F1 (Val set of Class-0): 0.9406\n",
      "F1 (Val set of Class-1): 0.9394\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter for SVM\n",
    "hyper_parameters_svm = {\n",
    "\n",
    "    # TODO: please choose different values to tune the model\n",
    "    'C': 1e-1# [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "\n",
    "}\n",
    "# obtain cross-validation set\n",
    "train_datas, train_labels, validation_datas, validation_labels = train_validation_split(K, train_validation_data, train_validation_label)\n",
    "\n",
    "\n",
    "for i, (train_data, train_label, validation_data, validation_label) in enumerate(zip(train_datas, train_labels, validation_datas, validation_labels)):\n",
    "\n",
    "    print(f'# ======================= {i + 1}-th time validation ======================= #')\n",
    "        # SVM\n",
    "\n",
    "    print(f'Algorithm: [SVM] =========================================')\n",
    "    print(f'hyper-parameter: {hyper_parameters_svm}')\n",
    "    svm_model = SVC(kernel='linear', **hyper_parameters_svm).fit(train_data, train_label)\n",
    "\n",
    "    # performance evaluation on validation set for tuning hyper-parameters\n",
    "    pred_label = svm_model.predict(validation_data)\n",
    "    F1_0 = eva_f1(validation_label, pred_label, _class=0)\n",
    "    \n",
    "    print(f'F1 (Val set of Class-0): {F1_0:.4f}')\n",
    "    F1_1 = eva_f1(validation_label, pred_label, _class=1)\n",
    "    \n",
    "    print(f'F1 (Val set of Class-1): {F1_1:.4f}')\n",
    "    f1_lst = [F1_0,F1_1]\n",
    "    fold_output[i].append(f1_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ======================= 1-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 1}\n",
      "F1 (Val set of Class-0): 0.9557\n",
      "F1 (Val set of Class-1): 0.9543\n",
      "# ======================= 2-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 1}\n",
      "F1 (Val set of Class-0): 0.9254\n",
      "F1 (Val set of Class-1): 0.9246\n",
      "# ======================= 3-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 1}\n",
      "F1 (Val set of Class-0): 0.9261\n",
      "F1 (Val set of Class-1): 0.9239\n",
      "# ======================= 4-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 1}\n",
      "F1 (Val set of Class-0): 0.9436\n",
      "F1 (Val set of Class-1): 0.9463\n",
      "# ======================= 5-th time validation ======================= #\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: {'C': 1}\n",
      "F1 (Val set of Class-0): 0.9490\n",
      "F1 (Val set of Class-1): 0.9510\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter for SVM\n",
    "hyper_parameters_svm = {\n",
    "\n",
    "    # TODO: please choose different values to tune the model\n",
    "    'C': 1# [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "\n",
    "}\n",
    "# obtain cross-validation set\n",
    "train_datas, train_labels, validation_datas, validation_labels = train_validation_split(K, train_validation_data, train_validation_label)\n",
    "\n",
    "\n",
    "for i, (train_data, train_label, validation_data, validation_label) in enumerate(zip(train_datas, train_labels, validation_datas, validation_labels)):\n",
    "\n",
    "    print(f'# ======================= {i + 1}-th time validation ======================= #')\n",
    "        # SVM\n",
    "\n",
    "    print(f'Algorithm: [SVM] =========================================')\n",
    "    print(f'hyper-parameter: {hyper_parameters_svm}')\n",
    "    svm_model = SVC(kernel='linear', **hyper_parameters_svm).fit(train_data, train_label)\n",
    "\n",
    "    # performance evaluation on validation set for tuning hyper-parameters\n",
    "    pred_label = svm_model.predict(validation_data)\n",
    "    F1_0 = eva_f1(validation_label, pred_label, _class=0)\n",
    "    \n",
    "    print(f'F1 (Val set of Class-0): {F1_0:.4f}')\n",
    "    F1_1 = eva_f1(validation_label, pred_label, _class=1)\n",
    "    \n",
    "    print(f'F1 (Val set of Class-1): {F1_1:.4f}')\n",
    "    f1_lst = [F1_0,F1_1]\n",
    "    fold_output[i].append(f1_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.9651741293532338, 0.964824120603015],\n",
       "  [0.9452736318407959, 0.9447236180904524],\n",
       "  [0.9285714285714285, 0.9313725490196078],\n",
       "  [0.9489795918367346, 0.9509803921568627],\n",
       "  [0.911917098445596, 0.9178743961352657],\n",
       "  [0.9556650246305418, 0.9543147208121826]],\n",
       " [[0.9154228855721394, 0.9145728643216081],\n",
       "  [0.9509803921568627, 0.9489795918367346],\n",
       "  [0.9552238805970149, 0.9547738693467336],\n",
       "  [0.9306930693069307, 0.9292929292929293],\n",
       "  [0.94, 0.94],\n",
       "  [0.9253731343283582, 0.9246231155778895]],\n",
       " [[0.9587628865979382, 0.9611650485436893],\n",
       "  [0.9547738693467336, 0.9552238805970149],\n",
       "  [0.9215686274509804, 0.9183673469387755],\n",
       "  [0.9543147208121826, 0.9556650246305418],\n",
       "  [0.9346733668341709, 0.9353233830845772],\n",
       "  [0.9261083743842364, 0.9238578680203046]],\n",
       " [[0.9702970297029702, 0.9696969696969697],\n",
       "  [0.97, 0.97],\n",
       "  [0.9595959595959594, 0.9603960396039604],\n",
       "  [0.9261083743842364, 0.9238578680203046],\n",
       "  [0.911764705882353, 0.9081632653061226],\n",
       "  [0.9435897435897437, 0.9463414634146342]],\n",
       " [[0.9547738693467336, 0.9552238805970149],\n",
       "  [0.96, 0.96],\n",
       "  [0.9500000000000001, 0.9500000000000001],\n",
       "  [0.9458128078817734, 0.9441624365482233],\n",
       "  [0.9405940594059405, 0.9393939393939393],\n",
       "  [0.9489795918367346, 0.9509803921568627]]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 1, 1]\n",
      "[1e-05, 0.001, 1e-05, 0.0001, 0.0001]\n",
      "Observed from above results, we know the optimal hyper-parameters that have largest f1 score for both classes in each fold/split are: [1e-05, 0.001, 1e-05, 0.0001, 0.0001]\n"
     ]
    }
   ],
   "source": [
    "C_lst = [1e-5,1e-4,1e-3,1e-2,1e-1,1]\n",
    "print(find_max_avg_indices(fold_output))\n",
    "C_ind = find_max_avg_indices(fold_output)\n",
    "C = [C_lst[i] for i in C_ind]\n",
    "print(C)\n",
    "print(f'Observed from above results, we know the optimal hyper-parameters that have largest f1 score for both classes in each fold/split are: {C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed from above results, we know the largest f1 score for both classes in each fold/split are: \n",
    "\n",
    "1st fold: 1e-5ï¼›\n",
    "2nd fold: 1e-5ï¼›\n",
    "3rd fold: 1e-5ï¼›\n",
    "4th fold: 1e-5ï¼›\n",
    "5th fold: 1e-2ï¼›\n",
    "\n",
    "C = [1e-5,1e-5,1e-5,1e-5,1e-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.4 Test set output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ======================= 1-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: l1\n",
      "Result Class 0 (Test set): {'precision': 0.9282051282051282, 'recall': 0.905, 'f1': 0.9164556962025316, 'accuracy': 0.9175, 'auroc': 0.947725}\n",
      "Result Class 1 (Test set): {'precision': 0.9073170731707317, 'recall': 0.93, 'f1': 0.9185185185185186, 'accuracy': 0.9175, 'auroc': 0.947725}\n",
      "\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: 1e-05\n",
      "Result Class 0 (Test set): {'precision': 0.953125, 'recall': 0.915, 'f1': 0.9336734693877552, 'accuracy': 0.935, 'auroc': 0.9532000000000002}\n",
      "Result Class 1 (Test set): {'precision': 0.9182692307692307, 'recall': 0.955, 'f1': 0.9362745098039216, 'accuracy': 0.935, 'auroc': 0.9532000000000002}\n",
      "\n",
      "\n",
      "# ======================= 2-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: l1\n",
      "Result Class 0 (Test set): {'precision': 0.9226804123711341, 'recall': 0.895, 'f1': 0.9086294416243655, 'accuracy': 0.91, 'auroc': 0.9391749999999999}\n",
      "Result Class 1 (Test set): {'precision': 0.8980582524271845, 'recall': 0.925, 'f1': 0.9113300492610837, 'accuracy': 0.91, 'auroc': 0.9391749999999999}\n",
      "\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: 0.001\n",
      "Result Class 0 (Test set): {'precision': 0.9282051282051282, 'recall': 0.905, 'f1': 0.9164556962025316, 'accuracy': 0.9175, 'auroc': 0.9444}\n",
      "Result Class 1 (Test set): {'precision': 0.9073170731707317, 'recall': 0.93, 'f1': 0.9185185185185186, 'accuracy': 0.9175, 'auroc': 0.9444}\n",
      "\n",
      "\n",
      "# ======================= 3-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: l2\n",
      "Result Class 0 (Test set): {'precision': 0.875, 'recall': 0.945, 'f1': 0.9086538461538461, 'accuracy': 0.905, 'auroc': 0.9624250000000002}\n",
      "Result Class 1 (Test set): {'precision': 0.9402173913043478, 'recall': 0.865, 'f1': 0.9010416666666665, 'accuracy': 0.905, 'auroc': 0.9624250000000002}\n",
      "\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: 1e-05\n",
      "Result Class 0 (Test set): {'precision': 0.9538461538461539, 'recall': 0.93, 'f1': 0.9417721518987343, 'accuracy': 0.9425, 'auroc': 0.96155}\n",
      "Result Class 1 (Test set): {'precision': 0.9317073170731708, 'recall': 0.955, 'f1': 0.9432098765432099, 'accuracy': 0.9425, 'auroc': 0.96155}\n",
      "\n",
      "\n",
      "# ======================= 4-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: l2\n",
      "Result Class 0 (Test set): {'precision': 0.893719806763285, 'recall': 0.925, 'f1': 0.9090909090909091, 'accuracy': 0.9075, 'auroc': 0.9620250000000001}\n",
      "Result Class 1 (Test set): {'precision': 0.9222797927461139, 'recall': 0.89, 'f1': 0.905852417302799, 'accuracy': 0.9075, 'auroc': 0.9620250000000001}\n",
      "\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: 0.0001\n",
      "Result Class 0 (Test set): {'precision': 0.9583333333333334, 'recall': 0.92, 'f1': 0.9387755102040817, 'accuracy': 0.94, 'auroc': 0.9529750000000001}\n",
      "Result Class 1 (Test set): {'precision': 0.9230769230769231, 'recall': 0.96, 'f1': 0.9411764705882353, 'accuracy': 0.94, 'auroc': 0.9529750000000001}\n",
      "\n",
      "\n",
      "# ======================= 5-th time validation ======================= #\n",
      "Algorithm: [logistic regression] =========================\n",
      "hyper-parameter: l1\n",
      "Result Class 0 (Test set): {'precision': 0.9246231155778895, 'recall': 0.92, 'f1': 0.9223057644110277, 'accuracy': 0.9225, 'auroc': 0.9566750000000002}\n",
      "Result Class 1 (Test set): {'precision': 0.9203980099502488, 'recall': 0.925, 'f1': 0.9226932668329176, 'accuracy': 0.9225, 'auroc': 0.9566750000000002}\n",
      "\n",
      "Algorithm: [SVM] =========================================\n",
      "hyper-parameter: 0.0001\n",
      "Result Class 0 (Test set): {'precision': 0.935, 'recall': 0.935, 'f1': 0.935, 'accuracy': 0.935, 'auroc': 0.9697}\n",
      "Result Class 1 (Test set): {'precision': 0.935, 'recall': 0.935, 'f1': 0.935, 'accuracy': 0.935, 'auroc': 0.9697}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# performance evaluation on test set\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "K = 5\n",
    "\n",
    "\n",
    "# hyper-parameter penlty for logistic regression. Hint: len(penalty) = 5\n",
    "\n",
    "penalty = [penalty_lst[i] for i in penalty_ind] # Please check above codes to check this list expression generated by function.\n",
    "\n",
    "# hyper-parameter C for SVM. Hint: len(C) = 5\n",
    "C = [C_lst[i] for i in C_ind] # Please check above codes to check this list expression generated by function.\n",
    "\n",
    "    \n",
    "# obtain training data\n",
    "train_datas, train_labels, _, _ = train_validation_split(K, train_validation_data, train_validation_label)\n",
    "\n",
    "\n",
    "for i, (train_data, train_label) in enumerate(zip(train_datas, train_labels)):\n",
    "\n",
    "    print(f'# ======================= {i + 1}-th time validation ======================= #')\n",
    "    hyper_parameters_logistic_regression = penalty[i]\n",
    "    hyper_parameters_svm = C[i]\n",
    "    # logistic regression\n",
    "\n",
    "    print(f'Algorithm: [logistic regression] =========================')\n",
    "    print(f'hyper-parameter: {hyper_parameters_logistic_regression}')\n",
    "    lr_model = LogisticRegression(solver='liblinear', penalty=penalty[i]).fit(train_data, train_label)\n",
    "\n",
    "\n",
    "    # performance evaluation on test set\n",
    "    pred_label = lr_model.predict(test_data)\n",
    "    results_0 = evaluation(test_label, pred_label, _class=0)\n",
    "    results_1 = evaluation(test_label, pred_label, _class=1)\n",
    "    print(f'Result Class 0 (Test set): {results_0}')\n",
    "    print(f'Result Class 1 (Test set): {results_1}')\n",
    "    print()\n",
    "    # SVM\n",
    "\n",
    "    print(f'Algorithm: [SVM] =========================================')\n",
    "    print(f'hyper-parameter: {hyper_parameters_svm}')\n",
    "    svm_model = SVC(kernel='linear', C=C[i]).fit(train_data, train_label)\n",
    "\n",
    "    # performance evaluation on test set\n",
    "    pred_label = svm_model.predict(test_data)\n",
    "    results_0 = evaluation(test_label, pred_label, _class=0)\n",
    "    results_1 = evaluation(test_label, pred_label, _class=1)\n",
    "    print(f'Result Class 0 (Test set): {results_0}')\n",
    "    print(f'Result Class 1 (Test set): {results_1}')\n",
    "\n",
    "    print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
